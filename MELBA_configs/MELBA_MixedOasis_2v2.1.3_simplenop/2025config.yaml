#reduce W_Rel to: 1 and W_Mse: 1 

#Alternatively, only train NOP fixed Unet

#learning rate: 0.0005
model:
  pretrained_checkpoint: /home/nellie/code/cvpr/BaseLine/DynamiCrafter/2025_SAVE_MELBA/MELBA_MixedOasis_2v2.1.3_simplenop/checkpoints/W0.5-epoch=489-step=4410.ckpt
  # pretrained_checkpoint: /home/nellie/code/cvpr/BaseLine/DynamiCrafter/2025_SAVE_MELBA/MELBA_MixedOasis_2v2.1.3_simplenop/checkpoints/w0.8-epoch=1539-step=13860.ckpt



  

  base_learning_rate: 1.0e-05
  scale_lr: False
  target: lvdm.modules.modules2D3D.MELBAgdnAlter.NN
  
  params:
    inshape: [128, 128, 128]
    alter_train_epoches: [100,2000]
    
  
    #3D
    gamma: 1.0
    alpha: 1.0
    weight_mse: 0.5
    weight_reg: 0.15

    # weight_sigma: 0.01
    weight_sigma: 0.03


    unet_lr: 0.0005
    nop_lr: 0.0005
    ReparametrizeVectorField: False
    PredictMomnetum: False
    
    
    unet_config:
      target: lvdm.modules.modules2D3D.unet.unet
      params:
        ndims: 3
        inshape: [128, 128, 128]  #img_size
        # nb_features: [[16, 32, 32, 32], [32, 32, 32, 32, 16]]
        # nb_features: [[8, 16], [16, 16, 8]]
        nb_features: [[8, 16, 16, 8], [8, 16, 16, 16, 8]]
        infeats: 2   #input channels
        
    nop_config:
      target: lvdm.modules.modules2D3D.nop.nop3dsimple
      params: 
        ModesFno: 4
        WidthFno: 8
        TSteps: 5
        layers: 2
  
    reg_config: 
      target: lvdm.metrics.regmetrics.MVMList
      # target: lvdm.metrics.regmetrics.MVGradList
      params:
        ndims: 3
        # penalty: 'l2'  #for MVGradList
    
    nop_loss_config:
      target: lvdm.metrics.regmetrics.LpLossMetricList
      params:
        W_Rel: 0.5
        W_Mse: 0.5



        


data:
  target: lvdm.data.2025data.DataModuleFromConfig
  params:
    batch_size: 6
    num_workers: 6
    
    train:
      target: lvdm.data.2025data_modules.MixedOasis
      params:
        path_OASIS1D: "/home/nellie/data/OASIS1_128/trainlist_dw.txt"
        path_OASIS3D: "/home/nellie/data/OASIS3-128/trainlist_dw.txt"
        split: "train"
        cur_img_size: 128
        img_size: 128
        pairs_per_epoch: 100

    valid:
      target: lvdm.data.2025data_modules.MixedOasis
      params:
        path_OASIS1D: "/home/nellie/data/OASIS1_128/validlist_dw2.txt"
        path_OASIS3D: "/home/nellie/data/OASIS3-128/validlist_dw2.txt"
        split: "valid"
        cur_img_size: 128
        img_size: 128
        pairs_per_epoch: 16

    # test:
    #   target: lvdm.data.2025data_modules.Oasis1d_LDDMM_Optimize
    #   params:
    #     path: "/home/nellie/data/OASIS1_128/testlist_dw2.txt"
    #     split: "test"
    #     idxpath: "/home/nellie/code/cvpr/BaseLine/DynamiCrafter/configs/2025training_oasis1d_v4.0_LDDMM_numerical_solver/test.json"
    #     cur_img_size: 128
    #     img_size: 128
    #     pairs_per_epoch: 100
    test:
      target: lvdm.data.2025data_modules.Oasis1d_LDDMM_Optimize
      params:
        path: "/home/nellie/data/OASIS1_128/test_valid_list_dw2.txt"
        split: "test"
        idxpath: "/home/nellie/code/cvpr/BaseLine/DynamiCrafter/configs/2025training_oasis1d_v4.0_LDDMM_numerical_solver/test.json"
        cur_img_size: 128
        img_size: 128
        pairs_per_epoch: 59
        # templateIDX: 5
        templateIDX: 10
        # templateIDX: 15
        # templateIDX: 20
        # templateIDX: 25
        # templateIDX: 30
        # templateIDX: 35
        # templateIDX: 40
        # templateIDX: 45
        # templateIDX: 50




lightning:
  precision: 32
  strategy: dp
  trainer:
    benchmark: True
    accumulate_grad_batches: 2
    max_epochs: 10000
    min_epochs: 500
    # logger
    log_every_n_steps: 50
    # val
    check_val_every_n_epoch: 2
    
    # gradient_clip_algorithm: 'norm'
    # gradient_clip_val: 0.5
  callbacks:
    model_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        every_n_epochs: 1 #1000
        filename: "{epoch}-{step}"
        save_weights_only: True
    metrics_val_loss_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        monitor: "val_loss"            # 监控的 metric
        mode: "min"                    # 保存最小值
        filename: "{epoch}-{val_loss:.4f}"
        save_top_k: 1                  # 只保留最优的检查点
        save_weights_only: True         # 只保存权重
    metrics_mse_loss_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        monitor: "val_mse"            # 监控的 metric
        mode: "min"                  # 保存最小值
        filename: "{epoch}-{val_mse:.4f}"
        save_top_k: 1                  # 只保留最优的检查点
        save_weights_only: True         # 只保存权重

