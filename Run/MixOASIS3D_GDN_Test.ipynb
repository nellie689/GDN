{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse, os, sys, datetime\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import logging as transf_logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "import torch\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from utils.utils import instantiate_from_config\n",
    "from utils.utils_train import get_trainer_callbacks, get_trainer_logger, get_trainer_strategy\n",
    "from utils.utils_train import set_logger, init_workspace, load_checkpoints, load_checkpoints_unet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_parser(**parser_kwargs):\n",
    "    parser = argparse.ArgumentParser(**parser_kwargs)\n",
    "\n",
    "    parser.add_argument(\"--seed\", \"-s\", type=int, default=20250101, help=\"seed for seed_everything\")\n",
    "    # parser.add_argument(\"--seed\", \"-s\", type=int, default=0, help=\"seed for seed_everything\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--name\", \"-n\", type=str, default=\"\", help=\"experiment name, as saving folder\")\n",
    "\n",
    "    parser.add_argument(\"--base\", \"-b\", nargs=\"*\", metavar=\"base_config.yaml\", help=\"paths to base configs. Loaded from left-to-right. \"\n",
    "                            \"Parameters can be overwritten or added with command-line options of the form `--key value`.\", default=list())\n",
    "    \n",
    "    parser.add_argument(\"--train\", \"-t\", action='store_true', default=False, help='train')\n",
    "    parser.add_argument(\"--val\", \"-v\", action='store_true', default=False, help='val')\n",
    "    parser.add_argument(\"--test\", action='store_true', default=False, help='test')\n",
    "\n",
    "    parser.add_argument(\"--logdir\", \"-l\", type=str, default=\"logs\", help=\"directory for logging dat shit\")\n",
    "    parser.add_argument(\"--auto_resume\", action='store_true', default=False, help=\"resume from full-info checkpoint\")\n",
    "    parser.add_argument(\"--auto_resume_weight_only\", action='store_true', default=False, help=\"resume from weight-only checkpoint\")\n",
    "    parser.add_argument(\"--debug\", \"-d\", action='store_true', default=False, help=\"enable post-mortem debugging\")\n",
    "    \n",
    "\n",
    "    return parser\n",
    "    \n",
    "def get_nondefault_trainer_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "    default_trainer_args = parser.parse_args([])\n",
    "    return sorted(k for k in vars(default_trainer_args) if getattr(args, k) != getattr(default_trainer_args, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = get_parser()\n",
    "# print(parser)\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "\n",
    "### test ####\n",
    "name=\"MixedOasis_Layer1\" #layers=1\n",
    "# name=\"MixedOasis_Layer2\" #layers=2\n",
    "# name=\"MixedOasis_Layer3\" #layers=3\n",
    "# name=\"MixedOasis_Layer4\" #layers=4\n",
    "# name=\"MixedOasis_Layer5\" #layers=5\n",
    "\n",
    "save_root = \"2025_SAVE_MELBA\"  # Replace with your save root directory\n",
    "HOST_GPU_NUM = 1\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()  # current work directory\n",
    "parent_directory = os.path.dirname(current_directory)  # parent work directory\n",
    "target_path = os.path.join(parent_directory, save_root, name)\n",
    "print(target_path)\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "config_file = f\"{parent_directory}/configs/{name}/2025config.yaml\"  # Replace with the actual config file path\n",
    "logdir = f\"{parent_directory}/{save_root}\"  # Replace with the actual log directory path\n",
    "\n",
    "\n",
    "args, unknown = parser.parse_known_args([\n",
    "    '--base', config_file,\n",
    "    '--train',\n",
    "    '--name', name,\n",
    "    '--logdir', logdir,\n",
    "    '--devices', str(HOST_GPU_NUM),\n",
    "    'lightning.trainer.num_nodes=1'\n",
    "])\n",
    "# print(args)\n",
    "# print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.base)\n",
    "print(current_directory)\n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "## disable transformer warning\n",
    "transf_logging.set_verbosity_error()\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yaml configs: \"model\" | \"data\" | \"lightning\"\n",
    "configs = [OmegaConf.load(cfg) for cfg in args.base]\n",
    "print(configs)\n",
    "cli = OmegaConf.from_dotlist(unknown)\n",
    "print(cli)\n",
    "print(len(configs))\n",
    "print(configs[0].keys())\n",
    "print(configs[0]['data']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.merge(*configs, cli)\n",
    "lightning_config = config.pop(\"lightning\", OmegaConf.create())\n",
    "trainer_config = lightning_config.get(\"trainer\", OmegaConf.create()) \n",
    "print(lightning_config)\n",
    "print(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rank=0\n",
    "print(args.name, args.logdir, config, lightning_config, global_rank)\n",
    "workdir, ckptdir, cfgdir, loginfo = init_workspace(args.name, args.logdir, config, lightning_config, global_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)\n",
    "print(type(config))\n",
    "print(config.model)\n",
    "print(config.data['params']['test']['params']['templateIDX'])\n",
    "\n",
    "test_data_params = config.data.get('params', {}).get('test', {}).get('params', {})\n",
    "if 'templateIDX' in test_data_params:\n",
    "    templateIDX = test_data_params['templateIDX']\n",
    "    config.model['params']['templateIDX'] = templateIDX\n",
    "else:\n",
    "    print(\"templateIDX not found\")\n",
    "\n",
    "print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup workspace directories\n",
    "logger = set_logger(logfile=os.path.join(loginfo, 'log_%d:%s.txt'%(global_rank, now)))\n",
    "logger.info(\"@lightning version: %s [>=1.8 required]\"%(pl.__version__))  \n",
    "## MODEL CONFIG >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "logger.info(\"***** Configing Model *****\")\n",
    "\n",
    "config.model.params.logdir = workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.model)\n",
    "model = instantiate_from_config(config.model)\n",
    "# assert 3>333\n",
    "## load checkpoints\n",
    "model = load_checkpoints(model, config.model)\n",
    "# model = load_checkpoints_unet(model, config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer_config)\n",
    "print(args)\n",
    "\n",
    "## update trainer config\n",
    "for k in get_nondefault_trainer_args(args):\n",
    "    trainer_config[k] = getattr(args, k)\n",
    "    print(k, getattr(args, k))\n",
    "\n",
    "num_nodes = trainer_config.num_nodes\n",
    "ngpu_per_node = trainer_config.devices\n",
    "num_rank=1\n",
    "logger.info(f\"Running on {num_rank}={num_nodes}x{ngpu_per_node} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA CONFIG >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "logger.info(\"***** Configing Data *****\")\n",
    "print(config.data)\n",
    "\n",
    "data = instantiate_from_config(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINER CONFIG >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "logger.info(\"***** Configing Trainer *****\")\n",
    "if \"accelerator\" not in trainer_config:\n",
    "    trainer_config[\"accelerator\"] = \"gpu\"\n",
    "\n",
    "print(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup trainer args: pl-logger and callbacks\n",
    "trainer_kwargs = dict()\n",
    "trainer_kwargs[\"num_sanity_val_steps\"] = 0\n",
    "logger_cfg = get_trainer_logger(lightning_config, workdir, args.debug)\n",
    "trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## setup callbacks\n",
    "callbacks_cfg = get_trainer_callbacks(lightning_config, config, workdir, ckptdir, logger)\n",
    "trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n",
    "strategy_cfg = get_trainer_strategy(lightning_config)\n",
    "trainer_kwargs[\"strategy\"] = strategy_cfg if type(strategy_cfg) == str else instantiate_from_config(strategy_cfg)\n",
    "trainer_kwargs['precision'] = lightning_config.get('precision', 32)\n",
    "trainer_kwargs[\"sync_batchnorm\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer_kwargs[\"callbacks\"])\n",
    "print(len(trainer_kwargs[\"callbacks\"]))\n",
    "print(callbacks_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.model.pretrained_checkpoint\n",
    "## trainer config: others\n",
    "trainer_args = argparse.Namespace(**trainer_config)\n",
    "trainer = Trainer.from_argparse_args(trainer_args, **trainer_kwargs)\n",
    "print(trainer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.train)\n",
    "print(\"strategy\" in lightning_config and lightning_config['strategy'].startswith('deepspeed'))\n",
    "assert 2>33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
